{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main path:  /home/billy/mental-llm/llm_chatbot_backend\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "main_path = os.getcwd()\n",
    "sys.path.append(main_path)\n",
    "print(\"Main path: \", main_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-05 17:25:41.170205\n"
     ]
    }
   ],
   "source": [
    "import huggingface_hub\n",
    "from datetime import datetime\n",
    "huggingface_hub.login(\"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\")\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import re\n",
    "from log import logger\n",
    "\n",
    "class MentalLlm():\n",
    "    def __init__(self, max_length=512, model_name=\"MentalLlm\", **kwags):\n",
    "        self.max_length = max_length\n",
    "        self.user_name = \"사용자\"\n",
    "        self.device = kwags.get('device', 'cpu')\n",
    "\n",
    "        # 모델과 토크나이저 로드\n",
    "        load_model_name = \"juhoon01/ko_llama3_model_shinhan_4\"\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(load_model_name, torch_dtype=torch.float32)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(load_model_name)\n",
    "\n",
    "        logger.info(f\"Load {model_name} model complete.\")\n",
    "\n",
    "    def preProcess(self, text: str) -> str:\n",
    "        \"\"\" LLM에 입력하기 전에 전처리를 수행\"\"\"\n",
    "        return text\n",
    "\n",
    "    def postProcess(self, text: str) -> str:\n",
    "        \"\"\" LLM의 출력을 후처리\"\"\"\n",
    "        text = text.replace(\"사우\", f\"{self.user_name}\")\n",
    "        text = re.sub(r'<\\|.*?\\|>', '', text)\n",
    "        text = re.sub(r'\\|endoftask\\|=1>', '', text)\n",
    "\n",
    "        # Remove text within angle brackets (e.g., <br>, <p>, </li>, etc.)\n",
    "        text = re.sub(r'<[^<>]*>', '', text)\n",
    "\n",
    "        # Remove incomplete tags or tokens starting with < or ending with >\n",
    "        text = re.sub(r'<[^ ]*', '', text)\n",
    "        text = re.sub(r'[^ ]*>', '', text)\n",
    "\n",
    "        # Remove special characters that may have been left behind\n",
    "        text = re.sub(r'[<>]', '', text)\n",
    "        #remove undefined\n",
    "        text = re.sub(r'undefined', '', text)\n",
    "\n",
    "        # Normalize whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "\n",
    "\n",
    "    def remove_repeated_sentences(self, text: str) -> str:\n",
    "        return text\n",
    "        \"\"\"텍스트에서 반복되는 문장을 제거\"\"\"\n",
    "        seen = set()\n",
    "        result = []\n",
    "        for sentence in text.split('. '):  # 문장을 '.' 기준으로 나눔\n",
    "            sentence = sentence.strip()   # 공백 제거\n",
    "            if sentence not in seen:\n",
    "                seen.add(sentence)\n",
    "                result.append(sentence)\n",
    "        return '. '.join(result)\n",
    "\n",
    "    def generate_response(self, prompt: str) -> str:\n",
    "        \"\"\" 사용자 입력에 대한 응답 생성 \"\"\"\n",
    "        # 프롬프트만 사용\n",
    "        full_prompt = f\"### 질문: {prompt}\\n### 답변:\"\n",
    "        inputs = self.tokenizer(full_prompt, return_tensors=\"pt\", truncation=True).to(self.device)\n",
    "\n",
    "        # 응답 생성\n",
    "        outputs = self.model.generate(\n",
    "            inputs.input_ids,\n",
    "            max_new_tokens=150,  # 더 많은 텍스트를 생성\n",
    "            temperature=0.7,\n",
    "            top_p=0.85,\n",
    "            top_k=50,\n",
    "            repetition_penalty=2.5,\n",
    "            no_repeat_ngram_size=3\n",
    "        )\n",
    "\n",
    "        # 생성된 텍스트 디코딩 및 후처리\n",
    "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        response = response.replace(\"<|endoftext|>\", \"\").strip()\n",
    "\n",
    "        # 반복 제거 (필요시)\n",
    "        response = self.postProcess(self.remove_repeated_sentences(response))\n",
    "\n",
    "        # 필요 없는 텍스트 제거\n",
    "        if \"### 질문:\" in response:\n",
    "            response = response.split(\"### 질문:\")[-1]  # 마지막 응답만 가져옴\n",
    "\n",
    "        return response\n",
    "\n",
    "\n",
    "    def set_max_length(self, max_length):\n",
    "        self.max_length = max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/billy/anaconda3/envs/llama2-7b/lib/python3.11/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/home/billy/anaconda3/envs/llama2-7b/lib/python3.11/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dc5f94bef714cf6966834f81658eb71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-05 17:25:46 - log - INFO - Load MentalLlm model complete.\n"
     ]
    }
   ],
   "source": [
    "mental_llm = MentalLlm(device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Response:  일어나서 나가기도 힘들고 잠에서 깨고싶지않아요 ### 답변: 사용자님은 아침에 일어나는 것이 너무 어렵다고 하시네요. 그래서 출근하기 전까지 계속해서 누워있거나, 늦게나가려고 하는 경우도 있다고 합니다. 사실 우리 몸이 깊은 수면을 취하지 못하면 다음날 피로감과 졸음증상으로 나타납니다.(수명 1~2시간) 이러한 현상을 '불규칙한 주기성'이라고 부르는데요., 이는 밤새도록 자는 것보다 더 많은 시간동안 낮 동안의 활동량이나 식습관 등 생활 패턴 때문에 생기는 것입니다.. 또 다른 원인 중 하나로는 스트레스입니다..스트레쓰를 받으면 신체적인 변화와 함께\n"
     ]
    }
   ],
   "source": [
    "# 테스트할 프롬프트\n",
    "prompt = \"일어나서 나가기도 힘들고 잠에서 깨고싶지않아요\"\n",
    "\n",
    "# generate_response 메서드를 이용해 모델 응답 생성\n",
    "response = mental_llm.generate_response(prompt)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"Model Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-05 17:27:10 - torch.distributed.nn.jit.instantiator - INFO - Created a temporary directory at /tmp/tmpzbx0wbj8\n",
      "2024-12-05 17:27:10 - torch.distributed.nn.jit.instantiator - INFO - Writing /tmp/tmpzbx0wbj8/_remote_module_non_scriptable.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/billy/anaconda3/envs/llama2-7b/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/billy/anaconda3/envs/llama2-7b/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.85` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Response:  안녕하세요 ### 답변: 네, 무엇이 궁금하신가요? #\n"
     ]
    }
   ],
   "source": [
    "prompt = \"안녕하세요\"\n",
    "response = mental_llm.generate_response(prompt)\n",
    "print(f\"Model Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MentalLlm(BaseAiModel):\n",
    "    def __init__(self, max_length=512, model_name=\"MentalLlm\", **kwargs):\n",
    "        self.max_length = max_length\n",
    "        self.user_name = kwargs.get(\"user_name\", \"사용자\")\n",
    "        self.device = kwargs.get(\"device\", \"cpu\")\n",
    "\n",
    "        # 모델과 토크나이저 로드\n",
    "        load_model_name = \"juhoon01/ko_llama3_model_shinhan_4\"\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(load_model_name, torch_dtype=torch.float32)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(load_model_name)\n",
    "\n",
    "        logger.info(f\"Load {model_name} model complete.\")\n",
    "\n",
    "    def set_user_name(self, user_name: str):\n",
    "        \"\"\"사용자 이름 설정\"\"\"\n",
    "        self.user_name = user_name\n",
    "\n",
    "    def preProcess(self, text: str) -> str:\n",
    "        text = text.strip()\n",
    "        return text\n",
    "\n",
    "    def postProcess(self, text: str) -> str:\n",
    "        text = text.replace(\"사우\", self.user_name)\n",
    "        return text\n",
    "\n",
    "    def remove_repeated_sentences(self, text: str) -> str:\n",
    "        seen = set()\n",
    "        result = []\n",
    "        for sentence in text.split(\". \"):\n",
    "            sentence = sentence.strip()\n",
    "            if sentence not in seen:\n",
    "                seen.add(sentence)\n",
    "                result.append(sentence)\n",
    "        return \". \".join(result)\n",
    "\n",
    "    def summarize_response(self, text: str, max_sentences: int = 3) -> str:\n",
    "        sentences = text.split(\". \")\n",
    "        return \". \".join(sentences[:max_sentences]) + (\"...\" if len(sentences) > max_sentences else \"\")\n",
    "\n",
    "    def generate_one_shot_prompt(self, example_question: str, example_answer: str, new_question: str) -> str:\n",
    "        prompt = f\"\"\"\n",
    "        ### 질문:\n",
    "        {example_question}\n",
    "        ### 답변:\n",
    "        {example_answer}\n",
    "\n",
    "        위의 예제와 유사한 방식으로, 주어진 질문에 대해 적절한 답변을 작성하십시오.\n",
    "        ### 질문:\n",
    "        {new_question}\n",
    "        ### 답변:\n",
    "        \"\"\"\n",
    "        return prompt\n",
    "\n",
    "    def generate_response(self, prompt: str, one_shot: bool = False, example_question: str = None, example_answer: str = None) -> str:\n",
    "        if one_shot and example_question and example_answer:\n",
    "            full_prompt = self.generate_one_shot_prompt(example_question, example_answer, prompt)\n",
    "        else:\n",
    "            full_prompt = f\"\"\"\n",
    "                제공된 데이터는 {prompt}에 대한 질문과 답변을 포함한다. \n",
    "                사용자 증상을 분석하고, 의사 진단, 진단 근거, 해결책을 제시하라.\n",
    "                의사 진단은 간결하게 작성하며, 진단 근거는 진단의 근거가 되는 증거를 제시하고, 해결책은 적절한 조치를 제시해야 한다.\n",
    "                의사 진단, 진단 근거, 해결책은 각각 줄내림(\\n)으로 구분하여라.\n",
    "            \"\"\"\n",
    "\n",
    "        inputs = self.tokenizer(full_prompt, return_tensors=\"pt\", truncation=True).to(self.device)\n",
    "\n",
    "        outputs = self.model.generate(\n",
    "            inputs.input_ids,\n",
    "            max_new_tokens=150,\n",
    "            temperature=0.7,\n",
    "            top_p=0.85,\n",
    "            top_k=50,\n",
    "            repetition_penalty=2.5,\n",
    "            no_repeat_ngram_size=3\n",
    "        )\n",
    "\n",
    "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        response = response.replace(\"<|endoftext|>\", \"\").strip()\n",
    "\n",
    "        response = self.postProcess(self.remove_repeated_sentences(response))\n",
    "        if \"### 질문:\" in response:\n",
    "            response = response.split(\"### 질문:\")[-1]\n",
    "\n",
    "        response = self.summarize_response(response)\n",
    "\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "MentalLlm.generate_response() got an unexpected keyword argument 'one_shot'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m new_question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m매일 밤 야근 때문에 수면 시간이 부족하고 피곤합니다. 어떻게 해야 할까요?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# 원샷 프롬프팅을 활용한 응답 생성\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mmental_llm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_question\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mone_shot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexample_question\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexample_question\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexample_answer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexample_answer\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# 결과 출력\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel Response: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: MentalLlm.generate_response() got an unexpected keyword argument 'one_shot'"
     ]
    }
   ],
   "source": [
    "# MentalLlm 객체 생성\n",
    "mental_llm = MentalLlm(device=\"cuda\")\n",
    "\n",
    "# 예제 질문과 답변\n",
    "example_question = \"요즘 회사에서 일이 많아 스트레스를 받고 있습니다. 잠도 잘 못 자고 몸 상태가 좋지 않습니다.\"\n",
    "example_answer = (\n",
    "    \"사용자님은 현재 회사에서 스트레스를 받고 계시다고 말씀하셨습니다. 매일 늦게까지 일하다가도 머리에서 일이 떠나지 않아 잠을 제대로 못 주무시고, \"\n",
    "    \"몸 상태도 악화되셨다고 하셨습니다. 이런 경우에는 일을 우선순위로 나누고 휴식 시간을 확보하는 것이 중요합니다. 명상, 운동, 여가 생활 등으로 스트레스를 해소해보세요.\"\n",
    ")\n",
    "\n",
    "# 새로운 질문\n",
    "new_question = \"매일 밤 야근 때문에 수면 시간이 부족하고 피곤합니다. 어떻게 해야 할까요?\"\n",
    "\n",
    "# 원샷 프롬프팅을 활용한 응답 생성\n",
    "response = mental_llm.generate_response(\n",
    "    prompt=new_question,\n",
    "    one_shot=True,\n",
    "    example_question=example_question,\n",
    "    example_answer=example_answer\n",
    ")\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"Model Response: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama2-7b",
   "language": "python",
   "name": "llama2-7b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
